1 输入为100维的随机噪声信号
2  生成器模型建立
       Generator(
  (l1): Sequential(
    (0): Linear(in_features=100, out_features=8192, bias=True)
  )
  (conv_blocks): Sequential(
    (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): Upsample(scale_factor=2.0, mode=nearest)
    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)
    (4): LeakyReLU(negative_slope=0.2, inplace=True)
    (5): Upsample(scale_factor=2.0, mode=nearest)
    (6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)
    (8): LeakyReLU(negative_slope=0.2, inplace=True)
    (9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (10): Tanh()
  )
)
先是输入100维的噪声信号，然后经过全链接层，输出为8192维的单列tensor，并且添加偏置。在卷积神经网络的卷积层之前和之后总会添加BatchNorm2d进行数据的归一化处理，这使得数据在进行Relu之前不会因为数据过大而导致网络性能的不稳定。先加一层batchnormal，当有参数可以定义训练的时候使nn.ConvTranspose2d，无参数的时候，可以采用自动插值的Upsample函数，两者功能一样。卷积块的操作：首先一个batchnormal，输入特征值为128，然后进行上采样，然后进行卷积操作，输入为128，输出为128，步长为1，padding为1，然后再经过一个batchnormal，归一化程度为0。8，是一个稳定系数。然后进入激活层，激活函数为leakyrelu函数，系数为0。2，inplace设置为True后,输出数据会覆盖输入数据,导致无法求取relu的梯度,一般在模型测试的时候,设置为True是没有问题的,但是在训练过程中,这样做就会导致梯度传递出现问题,所以会导致loss飙升,训练失败。然后卷积从128到64，再从64到1，通道数始终为3。最后经过一个tanh归一化到-1和1之间。



3 判别其模型建立
Discriminator(
  (model): Sequential(
    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Dropout2d(p=0.25, inplace=False)
    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (4): LeakyReLU(negative_slope=0.2, inplace=True)
    (5): Dropout2d(p=0.25, inplace=False)
    (6): BatchNorm2d(32, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)
    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (8): LeakyReLU(negative_slope=0.2, inplace=True)
    (9): Dropout2d(p=0.25, inplace=False)
    (10): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)
    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (12): LeakyReLU(negative_slope=0.2, inplace=True)
    (13): Dropout2d(p=0.25, inplace=False)
    (14): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)
  )
  (adv_layer): Sequential(
    (0): Linear(in_features=512, out_features=1, bias=True)
    (1): Sigmoid()
  )
)
判别器与生成器类似，把生成器生成的图片从1到16，经过一个卷积层，一个leakyrelu函数，然后丢掉百分之25的神经元个数。然后从16到32，32到64，64到128重复这个过程，然后经过一个全链接层和sigmoid函数。是这样一个套路

总的步骤为：G初始梯度归0，生成器生成一组图片，计算G的loss，表明生成器生成的图片骗过判别器的能力，然后更新G的权重。然后是G的初始梯度归0，判别器计算的是真实图片和生成图片的loss，然后去一个均值，更新D的权重，这样一张图片的生成和判别就完成了。
